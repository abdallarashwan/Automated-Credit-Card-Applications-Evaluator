{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Credit Card Applications Evaluator\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Manually evaluating credit card applications can be time-consuming and error-prone, motivated by that we will implement an automatic credit card application evaluator which predicts the acceptance or refusal using supervised machine learning algorithms.\n",
    "\n",
    "<p>We'll use the <a href=\"http://archive.ics.uci.edu/ml/datasets/credit+approval\">Credit Card Approval dataset</a> from the UCI Machine Learning Repository. </p>\n",
    "<ul>\n",
    "<li>First, we will start off performing Exploratory Data Analysis on the dataset.</li>\n",
    "<li>Next, we perform data cleaning and make sure to handle any fauly or missing data.</li>\n",
    "<li>Apply data transformations for preprocessing the data to be suitable for the machine learning models.</li>\n",
    "<li>Slicing and spliting the dataset into train and test data.</li>\n",
    "<li>Scaling the data into the appropriate range</li>\n",
    "<li> Checking correlation between features and target data.</li>\n",
    "<li> Training, hyperparameter tuning and evaluation of two different classification models.</li>\n",
    "</ul>\n",
    "<p>First, loading and viewing the dataset. We find that since this data is confidential, the contributor of the dataset has anonymized the columns names.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0      1      2  3  4  5  6     7  8  9   10 11 12     13   14 15\n",
      "0  b  30.83  0.000  u  g  w  v  1.25  t  t   1  f  g  00202    0  +\n",
      "1  a  58.67  4.460  u  g  q  h  3.04  t  t   6  f  g  00043  560  +\n",
      "2  a  24.50  0.500  u  g  q  h  1.50  t  f   0  f  g  00280  824  +\n",
      "3  b  27.83  1.540  u  g  w  v  3.75  t  t   5  t  g  00100    3  +\n",
      "4  b  20.17  5.625  u  g  w  v  1.71  t  f   0  f  s  00120    0  +\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cc_apps = pd.read_csv('datasets/cc_approvals.data', header = None )\n",
    "\n",
    "print(cc_apps.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA\n",
    "<p> The features of this dataset have been anonymized privacy reasons, but <a href=\"http://rstudio-pubs-static.s3.amazonaws.com/73039_9946de135c0a49daa7a0a9eda4a67a72.html\">this blog</a> gives us a pretty good overview of the probable features. The probable features in a typical credit card application are <code>Gender</code>, <code>Age</code>, <code>Debt</code>, <code>Married</code>, <code>BankCustomer</code>, <code>EducationLevel</code>, <code>Ethnicity</code>, <code>YearsEmployed</code>, <code>PriorDefault</code>, <code>Employed</code>, <code>CreditScore</code>, <code>DriversLicense</code>, <code>Citizen</code>, <code>ZipCode</code>, <code>Income</code> and finally the <code>ApprovalStatus</code>.</p>\n",
    "\n",
    "Next we explore the dataset more by getting the summery statistics using Pandas [describe( )](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) and [info( )](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html) methods as well as take a look at some rows at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               2           7          10             14\n",
      "count  690.000000  690.000000  690.00000     690.000000\n",
      "mean     4.758725    2.223406    2.40000    1017.385507\n",
      "std      4.978163    3.346513    4.86294    5210.102598\n",
      "min      0.000000    0.000000    0.00000       0.000000\n",
      "25%      1.000000    0.165000    0.00000       0.000000\n",
      "50%      2.750000    1.000000    0.00000       5.000000\n",
      "75%      7.207500    2.625000    3.00000     395.500000\n",
      "max     28.000000   28.500000   67.00000  100000.000000\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 690 entries, 0 to 689\n",
      "Data columns (total 16 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       690 non-null    object \n",
      " 1   1       690 non-null    object \n",
      " 2   2       690 non-null    float64\n",
      " 3   3       690 non-null    object \n",
      " 4   4       690 non-null    object \n",
      " 5   5       690 non-null    object \n",
      " 6   6       690 non-null    object \n",
      " 7   7       690 non-null    float64\n",
      " 8   8       690 non-null    object \n",
      " 9   9       690 non-null    object \n",
      " 10  10      690 non-null    int64  \n",
      " 11  11      690 non-null    object \n",
      " 12  12      690 non-null    object \n",
      " 13  13      690 non-null    object \n",
      " 14  14      690 non-null    int64  \n",
      " 15  15      690 non-null    object \n",
      "dtypes: float64(2), int64(2), object(12)\n",
      "memory usage: 86.4+ KB\n",
      "None\n",
      "\n",
      "\n",
      "    0      1       2  3  4   5   6      7  8  9   10 11 12     13   14 15\n",
      "670  b  47.17   5.835  u  g   w   v  5.500  f  f   0  f  g  00465  150  -\n",
      "671  b  25.83  12.835  u  g  cc   v  0.500  f  f   0  f  g  00000    2  -\n",
      "672  a  50.25   0.835  u  g  aa   v  0.500  f  f   0  t  g  00240  117  -\n",
      "673  ?  29.50   2.000  y  p   e   h  2.000  f  f   0  f  g  00256   17  -\n",
      "674  a  37.33   2.500  u  g   i   h  0.210  f  f   0  f  g  00260  246  -\n",
      "675  a  41.58   1.040  u  g  aa   v  0.665  f  f   0  f  g  00240  237  -\n",
      "676  a  30.58  10.665  u  g   q   h  0.085  f  t  12  t  g  00129    3  -\n",
      "677  b  19.42   7.250  u  g   m   v  0.040  f  t   1  f  g  00100    1  -\n",
      "678  a  17.92  10.210  u  g  ff  ff  0.000  f  f   0  f  g  00000   50  -\n",
      "679  a  20.08   1.250  u  g   c   v  0.000  f  f   0  f  g  00000    0  -\n",
      "680  b  19.50   0.290  u  g   k   v  0.290  f  f   0  f  g  00280  364  -\n",
      "681  b  27.83   1.000  y  p   d   h  3.000  f  f   0  f  g  00176  537  -\n",
      "682  b  17.08   3.290  u  g   i   v  0.335  f  f   0  t  g  00140    2  -\n",
      "683  b  36.42   0.750  y  p   d   v  0.585  f  f   0  f  g  00240    3  -\n",
      "684  b  40.58   3.290  u  g   m   v  3.500  f  f   0  t  s  00400    0  -\n",
      "685  b  21.08  10.085  y  p   e   h  1.250  f  f   0  f  g  00260    0  -\n",
      "686  a  22.67   0.750  u  g   c   v  2.000  f  t   2  t  g  00200  394  -\n",
      "687  a  25.25  13.500  y  p  ff  ff  2.000  f  t   1  t  g  00200    1  -\n",
      "688  b  17.92   0.205  u  g  aa   v  0.040  f  f   0  f  g  00280  750  -\n",
      "689  b  35.00   3.375  u  g   c   h  8.290  f  f   0  t  g  00000    0  -\n"
     ]
    }
   ],
   "source": [
    "# Print summary statistics\n",
    "cc_apps_description = cc_apps.describe()\n",
    "print(cc_apps_description)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print DataFrame information\n",
    "cc_apps_info = cc_apps.info()\n",
    "print(cc_apps_info)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Inspect the last 20 rows of the dataset\n",
    "print(cc_apps.tail(n = 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Some issues that can affect the performance of our machine learning model(s) include:</p>\n",
    "<ul>\n",
    "<li>Our dataset contains both numeric and non-numeric data, the columns 2, 7, 10 and 14 contain numeric values and all the other features contain non-numeric values.</li>\n",
    "<li>The dataset also contains values from several ranges.\n",
    "    \n",
    "<li>Finally, the dataset has missing values, which we'll take care of next. The missing values in the dataset are labeled with '?' and can be seen in row 673 and column 0</li>\n",
    "</ul>\n",
    "\n",
    "## 3. Finding the missing values \n",
    "\n",
    "<p>First, let's temporarily replace these missing value question marks with NaN.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0      1       2  3  4   5   6      7  8  9   10 11 12     13   14 15\n",
      "673  ?  29.50   2.000  y  p   e   h  2.000  f  f   0  f  g  00256   17  -\n",
      "674  a  37.33   2.500  u  g   i   h  0.210  f  f   0  f  g  00260  246  -\n",
      "675  a  41.58   1.040  u  g  aa   v  0.665  f  f   0  f  g  00240  237  -\n",
      "676  a  30.58  10.665  u  g   q   h  0.085  f  t  12  t  g  00129    3  -\n",
      "677  b  19.42   7.250  u  g   m   v  0.040  f  t   1  f  g  00100    1  -\n",
      "678  a  17.92  10.210  u  g  ff  ff  0.000  f  f   0  f  g  00000   50  -\n",
      "679  a  20.08   1.250  u  g   c   v  0.000  f  f   0  f  g  00000    0  -\n",
      "680  b  19.50   0.290  u  g   k   v  0.290  f  f   0  f  g  00280  364  -\n",
      "681  b  27.83   1.000  y  p   d   h  3.000  f  f   0  f  g  00176  537  -\n",
      "682  b  17.08   3.290  u  g   i   v  0.335  f  f   0  t  g  00140    2  -\n",
      "683  b  36.42   0.750  y  p   d   v  0.585  f  f   0  f  g  00240    3  -\n",
      "684  b  40.58   3.290  u  g   m   v  3.500  f  f   0  t  s  00400    0  -\n",
      "685  b  21.08  10.085  y  p   e   h  1.250  f  f   0  f  g  00260    0  -\n",
      "686  a  22.67   0.750  u  g   c   v  2.000  f  t   2  t  g  00200  394  -\n",
      "687  a  25.25  13.500  y  p  ff  ff  2.000  f  t   1  t  g  00200    1  -\n",
      "688  b  17.92   0.205  u  g  aa   v  0.040  f  f   0  f  g  00280  750  -\n",
      "689  b  35.00   3.375  u  g   c   h  8.290  f  f   0  t  g  00000    0  -\n",
      "\n",
      "\n",
      "      0      1       2  3  4   5   6      7  8  9   10 11 12     13   14 15\n",
      "673  NaN  29.50   2.000  y  p   e   h  2.000  f  f   0  f  g  00256   17  -\n",
      "674    a  37.33   2.500  u  g   i   h  0.210  f  f   0  f  g  00260  246  -\n",
      "675    a  41.58   1.040  u  g  aa   v  0.665  f  f   0  f  g  00240  237  -\n",
      "676    a  30.58  10.665  u  g   q   h  0.085  f  t  12  t  g  00129    3  -\n",
      "677    b  19.42   7.250  u  g   m   v  0.040  f  t   1  f  g  00100    1  -\n",
      "678    a  17.92  10.210  u  g  ff  ff  0.000  f  f   0  f  g  00000   50  -\n",
      "679    a  20.08   1.250  u  g   c   v  0.000  f  f   0  f  g  00000    0  -\n",
      "680    b  19.50   0.290  u  g   k   v  0.290  f  f   0  f  g  00280  364  -\n",
      "681    b  27.83   1.000  y  p   d   h  3.000  f  f   0  f  g  00176  537  -\n",
      "682    b  17.08   3.290  u  g   i   v  0.335  f  f   0  t  g  00140    2  -\n",
      "683    b  36.42   0.750  y  p   d   v  0.585  f  f   0  f  g  00240    3  -\n",
      "684    b  40.58   3.290  u  g   m   v  3.500  f  f   0  t  s  00400    0  -\n",
      "685    b  21.08  10.085  y  p   e   h  1.250  f  f   0  f  g  00260    0  -\n",
      "686    a  22.67   0.750  u  g   c   v  2.000  f  t   2  t  g  00200  394  -\n",
      "687    a  25.25  13.500  y  p  ff  ff  2.000  f  t   1  t  g  00200    1  -\n",
      "688    b  17.92   0.205  u  g  aa   v  0.040  f  f   0  f  g  00280  750  -\n",
      "689    b  35.00   3.375  u  g   c   h  8.290  f  f   0  t  g  00000    0  -\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 690 entries, 0 to 689\n",
      "Data columns (total 16 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       678 non-null    object \n",
      " 1   1       678 non-null    object \n",
      " 2   2       690 non-null    float64\n",
      " 3   3       684 non-null    object \n",
      " 4   4       684 non-null    object \n",
      " 5   5       681 non-null    object \n",
      " 6   6       681 non-null    object \n",
      " 7   7       690 non-null    float64\n",
      " 8   8       690 non-null    object \n",
      " 9   9       690 non-null    object \n",
      " 10  10      690 non-null    int64  \n",
      " 11  11      690 non-null    object \n",
      " 12  12      690 non-null    object \n",
      " 13  13      677 non-null    object \n",
      " 14  14      690 non-null    int64  \n",
      " 15  15      690 non-null    object \n",
      "dtypes: float64(2), int64(2), object(12)\n",
      "memory usage: 86.4+ KB\n",
      "None\n",
      "\n",
      "\n",
      "Total number of NaN variavbles 67\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(cc_apps.tail(n = 17))\n",
    "\n",
    "\n",
    "cc_apps.replace('?' , np.nan , inplace = True)\n",
    "print('\\n')\n",
    "\n",
    "print(cc_apps.tail(n = 17))\n",
    "print('\\n')\n",
    "\n",
    "print(cc_apps.info())\n",
    "print('\\n')\n",
    "print('Total number of NaN variavbles',sum(cc_apps.isna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see, we don't have any missing values in the columns with numerical values (2, 7, 10 and 14).\n",
    "\n",
    "- The total number of missing values is 67."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handling the missing values\n",
    "\n",
    "<p>We replaced all the question marks with NaNs.</p>\n",
    "\n",
    "There are missing values to be imputed for columns (0, 1, 3, 4, 5, 6 and 13). All of these columns contain non-numeric data.\n",
    "\n",
    "We are going to impute these missing values with the most frequent values as present in the respective columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of NaN variavbles: 0\n",
      "\n",
      "\n",
      "    0      1       2  3  4   5   6      7  8  9   10 11 12     13   14 15\n",
      "673  b  29.50   2.000  y  p   e   h  2.000  f  f   0  f  g  00256   17  -\n",
      "674  a  37.33   2.500  u  g   i   h  0.210  f  f   0  f  g  00260  246  -\n",
      "675  a  41.58   1.040  u  g  aa   v  0.665  f  f   0  f  g  00240  237  -\n",
      "676  a  30.58  10.665  u  g   q   h  0.085  f  t  12  t  g  00129    3  -\n",
      "677  b  19.42   7.250  u  g   m   v  0.040  f  t   1  f  g  00100    1  -\n",
      "678  a  17.92  10.210  u  g  ff  ff  0.000  f  f   0  f  g  00000   50  -\n",
      "679  a  20.08   1.250  u  g   c   v  0.000  f  f   0  f  g  00000    0  -\n",
      "680  b  19.50   0.290  u  g   k   v  0.290  f  f   0  f  g  00280  364  -\n",
      "681  b  27.83   1.000  y  p   d   h  3.000  f  f   0  f  g  00176  537  -\n",
      "682  b  17.08   3.290  u  g   i   v  0.335  f  f   0  t  g  00140    2  -\n",
      "683  b  36.42   0.750  y  p   d   v  0.585  f  f   0  f  g  00240    3  -\n",
      "684  b  40.58   3.290  u  g   m   v  3.500  f  f   0  t  s  00400    0  -\n",
      "685  b  21.08  10.085  y  p   e   h  1.250  f  f   0  f  g  00260    0  -\n",
      "686  a  22.67   0.750  u  g   c   v  2.000  f  t   2  t  g  00200  394  -\n",
      "687  a  25.25  13.500  y  p  ff  ff  2.000  f  t   1  t  g  00200    1  -\n",
      "688  b  17.92   0.205  u  g  aa   v  0.040  f  f   0  f  g  00280  750  -\n",
      "689  b  35.00   3.375  u  g   c   h  8.290  f  f   0  t  g  00000    0  -\n"
     ]
    }
   ],
   "source": [
    "for col in cc_apps.columns:\n",
    "    if cc_apps[col].dtypes == 'object':\n",
    "        cc_apps[col] = cc_apps[col].fillna(cc_apps[col].value_counts().index[0])\n",
    "\n",
    "print('Total number of NaN variavbles:',sum(cc_apps.isna().sum()))\n",
    "print('\\n')\n",
    "print(cc_apps.tail(17))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing the data (converting into numeric data) \n",
    "\n",
    "In this step, we will be converting all values into numeric ones which is essential when using SciKit-learn models. \n",
    "\n",
    "This can be done using [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "for col in cc_apps.columns.to_numpy():\n",
    "    if cc_apps[col].dtypes == 'object':\n",
    "        cc_apps[col]=encoder.fit_transform(cc_apps[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0    1      2   3   4   5   6     7   8   9   10  11  12  13   14  15\n",
      "0   1  156  0.000   1   0  12   7  1.25   1   1   1   0   0  68    0   0\n",
      "1   0  328  4.460   1   0  10   3  3.04   1   1   6   0   0  11  560   0\n",
      "2   0   89  0.500   1   0  10   3  1.50   1   0   0   0   0  96  824   0\n",
      "3   1  125  1.540   1   0  12   7  3.75   1   1   5   1   0  31    3   0\n",
      "4   1   43  5.625   1   0  12   7  1.71   1   0   0   0   2  37    0   0\n"
     ]
    }
   ],
   "source": [
    "print(cc_apps.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1       2   3   4   5   6     7   8   9   10  11  12  13   14  15\n",
      "685   1   52  10.085   2   2   4   3  1.25   0   0   0   0   0  90    0   1\n",
      "686   0   71   0.750   1   0   1   7  2.00   0   1   2   1   0  67  394   1\n",
      "687   0   97  13.500   2   2   5   2  2.00   0   1   1   1   0  67    1   1\n",
      "688   1   20   0.205   1   0   0   7  0.04   0   0   0   0   0  96  750   1\n",
      "689   1  197   3.375   1   0   1   3  8.29   0   0   0   1   0   0    0   1\n"
     ]
    }
   ],
   "source": [
    "print(cc_apps.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Splitting the dataset into train and test sets\n",
    "\n",
    "Now we need to split our dataset into two different sets as following:\n",
    "\n",
    "- train set: for training and fine tuning our models.\n",
    "- test set: for testing and evaluating the performance of the models.\n",
    "\n",
    "Before that, we need to drop the features that aren't important, those feautres are `DriversLicense` and `ZipCode` \n",
    "which are represented in columns number 12 and 13 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0    1      2   3   4   5   6     7   8   9   10  12   14  15\n",
      "0   1  156  0.000   1   0  12   7  1.25   1   1   1   0    0   0\n",
      "1   0  328  4.460   1   0  10   3  3.04   1   1   6   0  560   0\n",
      "2   0   89  0.500   1   0  10   3  1.50   1   0   0   0  824   0\n",
      "3   1  125  1.540   1   0  12   7  3.75   1   1   5   0    3   0\n",
      "4   1   43  5.625   1   0  12   7  1.71   1   0   0   2    0   0\n",
      "\n",
      "\n",
      "   0    1      2   3   4   5   6     7   8   9   10  12   14\n",
      "0   1  156  0.000   1   0  12   7  1.25   1   1   1   0    0\n",
      "1   0  328  4.460   1   0  10   3  3.04   1   1   6   0  560\n",
      "2   0   89  0.500   1   0  10   3  1.50   1   0   0   0  824\n",
      "3   1  125  1.540   1   0  12   7  3.75   1   1   5   0    3\n",
      "4   1   43  5.625   1   0  12   7  1.71   1   0   0   2    0\n",
      "\n",
      "\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: 15, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cc_apps = cc_apps.drop([11, 13], axis=1)\n",
    "\n",
    "print(cc_apps.head())\n",
    "print('\\n')\n",
    "\n",
    "X = cc_apps.loc[ : , 0:14]\n",
    "Y = cc_apps.loc[ : , 15]\n",
    "\n",
    "print(X.head())\n",
    "print('\\n')\n",
    "print(Y.head())\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split( X, Y, test_size = 0.3, random_state= 11 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Preprocessing the Data (scaling)\n",
    "\n",
    "The last preprocessing step we need to perform is scaling all our data to be in the same range.\n",
    "\n",
    "This is important in order to remove any bias that can happen due to some features having larger values compared to others.\n",
    "\n",
    "Next we use the [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) to scale all the input data in 0 to 1 range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=( 0 , 1 ))\n",
    "scaled_X_train = min_max_scaler.fit_transform(X_train)\n",
    "scaled_X_test = min_max_scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Selection\n",
    "\n",
    "Our goal is to predict if a credit card application will be accepted or not which makes it a classification problem.\n",
    "\n",
    "Before deciding which model to use for our case of classification, let's check if our features (input data) are correlated to the target (output data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0.028934\n",
      "1    -0.166966\n",
      "2    -0.206294\n",
      "3     0.194306\n",
      "4     0.185134\n",
      "5    -0.130434\n",
      "6    -0.000866\n",
      "7    -0.322475\n",
      "8    -0.720407\n",
      "9    -0.458301\n",
      "10   -0.406410\n",
      "12    0.100867\n",
      "14   -0.175657\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(cc_apps.loc[ : , : 14].corrwith(cc_apps[15]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the correlation matrix; our data shows a decent amount of correlation.\n",
    "\n",
    "Possible models to use in this case inclue: [RidgeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html) and [KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "ridge_model = RidgeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Tuning (RidgeClassifier)\n",
    "\n",
    "Next, we are going to use [GridSearchCV()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to tune our model of choice.\n",
    "\n",
    "We will also perform cross validation on five folds by passing the __cv__ argument.\n",
    "\n",
    "Hyperparameter in this case:\n",
    "\n",
    "- `alpha`: Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "alphas = {'alpha': np.array([ 0.001 , 0.01 , 0.1 , 0.2 , 0.4 , 0.8 , 1])}\n",
    "ridge_grid = GridSearchCV( ridge_model , alphas , cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Training (RidgeClassifier)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_trained_model = ridge_grid.fit(scaled_X_train , Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Evaluation (RidgeClassifier)\n",
    "\n",
    "Next step, we will use the test data to make predictions using the trained `RidgeClassifier` model.\n",
    "\n",
    "Then the performance of the model will be evaluated by calculating the [Classification Report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) and the [Confusion Matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "[[ 80   5]\n",
      " [ 21 101]]\n",
      "\n",
      "\n",
      "Classification report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.94      0.86        85\n",
      "           1       0.95      0.83      0.89       122\n",
      "\n",
      "    accuracy                           0.87       207\n",
      "   macro avg       0.87      0.88      0.87       207\n",
      "weighted avg       0.89      0.87      0.88       207\n",
      "\n",
      "\n",
      "\n",
      "Accuracy :\n",
      "0.8743961352657005\n",
      "\n",
      "\n",
      "Hyperparameter (alpha) :\n",
      "{'alpha': 0.001}\n",
      "\n",
      "\n",
      "Cross validation accuracy :\n",
      "0.8488187285223369\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "Y_preds = ridge_trained_model.predict(scaled_X_test)\n",
    "\n",
    "confMatrixRidge = confusion_matrix(Y_test , Y_preds)\n",
    "print(\"Confusion Matrix :\")\n",
    "print(confMatrixRidge)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "classificationReportRidge = classification_report(Y_test , Y_preds)\n",
    "print('Classification report :')\n",
    "print(classificationReportRidge)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"Accuracy :\")\n",
    "print(ridge_trained_model.score(scaled_X_test , Y_test))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"Hyperparameter (alpha) :\")\n",
    "print(ridge_trained_model.best_params_)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"Cross validation accuracy :\")\n",
    "print(ridge_trained_model.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "kn_model = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Hyperparameter Tuning (KNeighborsClassifier)\n",
    "\n",
    "Hyperparameter in this case:\n",
    "\n",
    "- `n_neighbors`: the number of neighbors to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = {'n_neighbors': np.arange(1, 50)}\n",
    "kn_grid = GridSearchCV(kn_model , neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Model Training (KNeighborsClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn_trained_model = kn_grid.fit(scaled_X_train , Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Model Evaluation (KNeighborsClassifier)\n",
    "\n",
    "Once again, we will use the test data to make predictions using the`KNeighbors` trained model.\n",
    "\n",
    "Then the performance of the model will be evaluated by calculating the [Classification Report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) and the [Confusion Matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "[[ 76   9]\n",
      " [ 14 108]]\n",
      "\n",
      "\n",
      "Classification report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.89      0.87        85\n",
      "           1       0.92      0.89      0.90       122\n",
      "\n",
      "    accuracy                           0.89       207\n",
      "   macro avg       0.88      0.89      0.89       207\n",
      "weighted avg       0.89      0.89      0.89       207\n",
      "\n",
      "\n",
      "\n",
      "Accuracy :\n",
      "0.8888888888888888\n",
      "\n",
      "\n",
      "Hyperparameter :\n",
      "{'n_neighbors': 30}\n",
      "\n",
      "\n",
      "Cross validation accuracy :\n",
      "0.8633161512027492\n"
     ]
    }
   ],
   "source": [
    "Y_preds = kn_trained_model.predict(scaled_X_test)\n",
    "\n",
    "confMatrixRidge = confusion_matrix(Y_test , Y_preds)\n",
    "print(\"Confusion Matrix :\")\n",
    "print(confMatrixRidge)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "classificationReportRidge = classification_report(Y_test , Y_preds)\n",
    "print('Classification report :')\n",
    "print(classificationReportRidge)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"Accuracy :\")\n",
    "print(kn_trained_model.score(scaled_X_test , Y_test))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"Hyperparameter :\")\n",
    "print(kn_trained_model.best_params_)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"Cross validation accuracy :\")\n",
    "print(kn_trained_model.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
